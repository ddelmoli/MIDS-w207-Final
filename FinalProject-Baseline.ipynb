{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'irish', u'mexican', u'chinese', u'filipino', u'vietnamese', u'moroccan', u'brazilian', u'japanese', u'british', u'greek', u'indian', u'jamaican', u'french', u'spanish', u'russian', u'cajun_creole', u'thai', u'southern_us', u'korean', u'italian']\n",
      "(39774,)\n"
     ]
    }
   ],
   "source": [
    "with open('train.json') as json_train_data:\n",
    "    train_raw = json.load(json_train_data)\n",
    "    \n",
    "with open('test.json') as json_test_data:\n",
    "    test_raw = json.load(json_test_data)\n",
    "    \n",
    "train_labels = []\n",
    "train_data = []\n",
    "\n",
    "for recipe in train_raw:\n",
    "    train_labels.append(recipe[\"cuisine\"])\n",
    "    ingredient_list = \" \".join([x.replace(\" \",\"_\") for x in recipe[\"ingredients\"]])\n",
    "    ingredient_list = re.sub(r'[^A-Za-z\\s_]', '', ingredient_list)\n",
    "    ingredient_list = re.sub(r'_+oz_', '', ingredient_list)\n",
    "    ingredient_list = re.sub(r' _+', '', ingredient_list)\n",
    "    train_data.append(ingredient_list)\n",
    "\n",
    "target_names = list(set(train_labels))\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "test_ids = []\n",
    "test_data = []\n",
    "\n",
    "for recipe in test_raw:\n",
    "    test_ids.append(recipe[\"id\"])\n",
    "    ingredient_list = \" \".join([x.replace(\" \",\"_\") for x in recipe[\"ingredients\"]])\n",
    "    ingredient_list = re.sub(r'[^A-Za-z\\s_]', '', ingredient_list)\n",
    "    ingredient_list = re.sub(r'_+oz_', '', ingredient_list)\n",
    "    ingredient_list = re.sub(r' _+', '', ingredient_list)\n",
    "    test_data.append(ingredient_list)\n",
    "\n",
    "print target_names\n",
    "print train_labels.shape\n",
    "\n",
    "mini_train_data = train_data[:7000]\n",
    "mini_train_labels = train_labels[:7000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'_lowfat_cottage_cheese', u'_lowfat_milk', u'_reducedfat_milk', u'a_taste_of_thai_rice_noodles', u'aai', u'abalone', u'abbamele', u'accent', u'accent_seasoning', u'achiote']\n"
     ]
    }
   ],
   "source": [
    "train_data = mini_train_data\n",
    "train_labels = mini_train_labels\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "train_docterm = vectorizer.fit_transform(train_data)\n",
    "\n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "print sorted(features)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.696142857143\n"
     ]
    }
   ],
   "source": [
    "train_data = mini_train_data\n",
    "train_labels = mini_train_labels\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "train_docterm = vectorizer.fit_transform(train_data)\n",
    "\n",
    "c = pow(2.0,np.arange(6)-6)\n",
    "parameters = {'C': c}\n",
    "lr = LogisticRegression()\n",
    "lr_clf = GridSearchCV(lr,parameters,scoring='accuracy')\n",
    "lr_clf.fit(train_docterm, train_labels)\n",
    "print lr_clf.best_score_\n",
    "\n",
    "#alpha = pow(2.0,np.arange(24)-12)\n",
    "#parameters = {'alpha': alpha}\n",
    "#mnb = MultinomialNB()\n",
    "#mnb_clf = GridSearchCV(mnb,parameters,scoring='accuracy')\n",
    "#mnb_clf.fit(train_docterm, train_labels)\n",
    "#print mnb_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 C:    0.01562  Vocabulary Size:    33  L2 Accuracy: 0.45657  L2 C:    0.50000\n",
      "L1 C:    0.03125  Vocabulary Size:    72  L2 Accuracy: 0.54571  L2 C:    0.50000\n",
      "L1 C:    0.06250  Vocabulary Size:   159  L2 Accuracy: 0.62743  L2 C:    0.50000\n",
      "L1 C:    0.12500  Vocabulary Size:   293  L2 Accuracy: 0.66914  L2 C:    0.50000\n",
      "L1 C:    0.25000  Vocabulary Size:   502  L2 Accuracy: 0.69057  L2 C:    0.50000\n",
      "L1 C:    0.50000  Vocabulary Size:   800  L2 Accuracy: 0.69886  L2 C:    0.50000\n",
      "L1 C:    1.00000  Vocabulary Size:  1250  L2 Accuracy: 0.70114  L2 C:    0.50000\n",
      "L1 C:    2.00000  Vocabulary Size:  2198  L2 Accuracy: 0.69843  L2 C:    0.50000\n",
      "L1 C:    4.00000  Vocabulary Size:  2505  L2 Accuracy: 0.69643  L2 C:    0.50000\n",
      "L1 C:    8.00000  Vocabulary Size:  2685  L2 Accuracy: 0.69714  L2 C:    0.50000\n",
      "L1 C:   16.00000  Vocabulary Size:  2850  L2 Accuracy: 0.69629  L2 C:    0.50000\n",
      "L1 C:   32.00000  Vocabulary Size:  3030  L2 Accuracy: 0.69614  L2 C:    0.50000\n"
     ]
    }
   ],
   "source": [
    "train_data = mini_train_data\n",
    "train_labels = mini_train_labels\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "train_docterm = vectorizer.fit_transform(train_data)\n",
    "total_vocab = vectorizer.vocabulary_\n",
    "total_vocab_byidx = dict((v,k) for k,v in vectorizer.vocabulary_.iteritems())\n",
    "\n",
    "vocab_sizes = []\n",
    "accuracies = []\n",
    "\n",
    "for c_l1 in pow(2.0,np.arange(12)-6):\n",
    "\n",
    "    # Train LogisticRegression with L1 penalty and specific C\n",
    "    lr_l1 = LogisticRegression(penalty='l1', tol=.01, C=c_l1)\n",
    "    lr_l1.fit(train_docterm, train_labels)\n",
    "\n",
    "    # Create a pruned vocabulary based on non-zero features from LogisticRegression with L1 penalty\n",
    "    pruned_vocab = set()\n",
    "    for f in lr_l1.coef_:\n",
    "        pruned_vocab.update([total_vocab_byidx[i] for i in np.flatnonzero(f)])\n",
    "\n",
    "    # Create a pruning vectorizer\n",
    "    pruning_vectorizer = CountVectorizer(vocabulary=pruned_vocab)\n",
    "    pruned_train_docterm = pruning_vectorizer.fit_transform(train_data)\n",
    "\n",
    "    # Train LogisticRegression with L2 penalty and pruned vocabulary\n",
    "    c = pow(2.0,np.arange(12)-12)\n",
    "    parameters = {'penalty' : ['l2'], 'C': c}\n",
    "    lr = LogisticRegression()\n",
    "    lr_clf = GridSearchCV(lr,parameters,scoring='accuracy')\n",
    "    lr_clf.fit(pruned_train_docterm, train_labels)\n",
    "    c_l2 = lr_clf.best_params_['C']\n",
    "\n",
    "    print \"L1 C: %10.5f  Vocabulary Size: %5d  L2 Accuracy: %.5f  L2 C: %10.5f\" % (c_l1, len(pruned_vocab), lr_clf.best_score_, c_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = mini_train_data\n",
    "train_labels = mini_train_labels\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "train_docterm = vectorizer.fit_transform(train_data)\n",
    "total_vocab = vectorizer.vocabulary_\n",
    "total_vocab_byidx = dict((v,k) for k,v in vectorizer.vocabulary_.iteritems())\n",
    "\n",
    "lr_l1 = LogisticRegression(penalty='l1', tol=.01, C=1.0)\n",
    "lr_l1.fit(train_docterm, train_labels)\n",
    "\n",
    "pruned_vocab = set()\n",
    "for f in lr_l1.coef_:\n",
    "    pruned_vocab.update([total_vocab_byidx[i] for i in np.flatnonzero(f)])\n",
    "\n",
    "pruning_vectorizer = CountVectorizer(vocabulary=pruned_vocab)\n",
    "pruned_train_docterm = pruning_vectorizer.fit_transform(train_data)\n",
    "pruned_test_docterm = pruning_vectorizer.transform(test_data)\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', C=0.5)\n",
    "lr.fit(pruned_train_docterm, train_labels)\n",
    "\n",
    "with open('submission.csv', 'wb') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(['id', 'cuisine'])\n",
    "    for i in range(len(test_data)):\n",
    "        csvwriter.writerow([test_ids[i], \" \".join(lr.predict(pruned_test_docterm[i])).strip()])\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py2]",
   "language": "python",
   "name": "conda-env-py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
