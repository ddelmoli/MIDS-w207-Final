{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GMM\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_ids, submission_data = [], []\n",
    "test_data, test_labels = [], []\n",
    "train_data, train_labels = [], []\n",
    "mini_train_data, mini_train_labels = [], []\n",
    "full_train_labels, full_train_data = [], []\n",
    "target_names, target_labels = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'irish', u'mexican', u'chinese', u'filipino', u'vietnamese', u'moroccan', u'brazilian', u'japanese', u'british', u'greek', u'indian', u'jamaican', u'french', u'spanish', u'russian', u'cajun_creole', u'thai', u'southern_us', u'korean', u'italian']\n",
      "(19887,)\n"
     ]
    }
   ],
   "source": [
    "def parse_all_data(per_recipe_function=None):\n",
    "    global submission_ids, submission_data, test_data, test_labels, train_data, train_labels\n",
    "    global mini_train_data, mini_train_labels, full_train_labels, full_train_data, target_names, target_labels\n",
    "    def parse_data(key_name, raw_data):\n",
    "        keys, data = [], []\n",
    "        for recipe in raw_data:\n",
    "            keys.append(recipe[key_name])\n",
    "            #ingredient_list = \" \".join([x.replace(\" \",\"_\") for x in recipe[\"ingredients\"]])\n",
    "            ingredient_list = \" \".join([x for x in recipe[\"ingredients\"]])\n",
    "            ingredient_list = re.sub(r'[^A-Za-z\\s_]', '', ingredient_list)\n",
    "            #ingredient_list = re.sub(r'_+oz_', '', ingredient_list)\n",
    "            #ingredient_list = re.sub(r' _+', '', ingredient_list)\n",
    "            # This adds the word *count* to the list of ingredients equal to the number of ingredients\n",
    "            # this results in a vectorization that includes a last feature that has a count\n",
    "            # equal to the number of ingredients.\n",
    "            ingredient_list += (\" *count*\" * len(ingredient_list.split(\" \")))  \n",
    "\n",
    "            if per_recipe_function is not None:\n",
    "                ingredient_list = per_recipe_function(ingredient_list)\n",
    "\n",
    "            data.append(ingredient_list)\n",
    "        return keys, data\n",
    "\n",
    "    with open('train.json') as json_train_data:\n",
    "        train_raw = json.load(json_train_data)\n",
    "\n",
    "    with open('test.json') as json_test_data:\n",
    "        test_raw = json.load(json_test_data)\n",
    "\n",
    "    full_train_labels, full_train_data = parse_data(\"cuisine\", train_raw)\n",
    "\n",
    "    target_names = list(set(full_train_labels))\n",
    "    full_train_labels = np.array(full_train_labels)\n",
    "\n",
    "    submission_ids, submission_data = parse_data(\"id\", test_raw)\n",
    "\n",
    "    num_test = len(full_train_labels)\n",
    "    test_data, test_labels = full_train_data[num_test/2:], full_train_labels[num_test/2:]\n",
    "    train_data, train_labels = full_train_data[:num_test/2], full_train_labels[:num_test/2]\n",
    "\n",
    "    mini_train_data = train_data[:7000]\n",
    "    mini_train_labels = train_labels[:7000]\n",
    "    \n",
    "parse_all_data()\n",
    "\n",
    "print target_names\n",
    "print train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a balanced training set that has an equal number of recipes per cuisine so no one cuisine dominates our training\n",
    "ftd = np.array(full_train_data)\n",
    "ftl = np.array(full_train_labels)\n",
    "\n",
    "balanced_train_data = np.array([])\n",
    "balanced_train_labels = np.array([])\n",
    "\n",
    "recipes_per_cuisine = 300\n",
    "for cuisine in target_names:\n",
    "    balanced_train_data = np.append(balanced_train_data,ftd[ftl==cuisine][:recipes_per_cuisine])\n",
    "    balanced_train_labels = np.append(balanced_train_labels,[cuisine for i in range(recipes_per_cuisine)])\n",
    "    \n",
    "# Initial results using \"balanced\" data are not encouraging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'aai', u'abalone', u'abbamele', u'acai', u'accent', u'achiote', u'ackee', u'acorn', u'active', u'added']\n"
     ]
    }
   ],
   "source": [
    "this_train_data = mini_train_data\n",
    "this_train_labels = mini_train_labels\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "train_docterm = vectorizer.fit_transform(this_train_data)\n",
    "\n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "print sorted(features)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.698571428571\n"
     ]
    }
   ],
   "source": [
    "this_train_data = mini_train_data\n",
    "this_train_labels = mini_train_labels\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "train_docterm = vectorizer.fit_transform(this_train_data)\n",
    "\n",
    "c = pow(2.0,np.arange(6)-6)\n",
    "parameters = {'C': c}\n",
    "lr = LogisticRegression()\n",
    "lr_clf = GridSearchCV(lr,parameters,scoring='accuracy')\n",
    "lr_clf.fit(train_docterm, this_train_labels)\n",
    "print lr_clf.best_score_\n",
    "\n",
    "#alpha = pow(2.0,np.arange(24)-12)\n",
    "#parameters = {'alpha': alpha}\n",
    "#mnb = MultinomialNB()\n",
    "#mnb_clf = GridSearchCV(mnb,parameters,scoring='accuracy')\n",
    "#mnb_clf.fit(train_docterm, train_labels)\n",
    "#print mnb_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 C:    0.01562  Vocabulary Size:    32  L2 Accuracy: 0.45814  L2 C:    0.50000\n",
      "L1 C:    0.03125  Vocabulary Size:    74  L2 Accuracy: 0.55314  L2 C:    0.50000\n",
      "L1 C:    0.06250  Vocabulary Size:   169  L2 Accuracy: 0.63157  L2 C:    0.50000\n",
      "L1 C:    0.12500  Vocabulary Size:   311  L2 Accuracy: 0.67200  L2 C:    0.50000\n",
      "L1 C:    0.25000  Vocabulary Size:   521  L2 Accuracy: 0.68986  L2 C:    0.50000\n",
      "L1 C:    0.50000  Vocabulary Size:   832  L2 Accuracy: 0.69943  L2 C:    0.50000\n",
      "L1 C:    1.00000  Vocabulary Size:  1297  L2 Accuracy: 0.70314  L2 C:    0.50000\n",
      "L1 C:    2.00000  Vocabulary Size:  2216  L2 Accuracy: 0.70186  L2 C:    0.50000\n",
      "L1 C:    4.00000  Vocabulary Size:  2499  L2 Accuracy: 0.70100  L2 C:    0.50000\n",
      "L1 C:    8.00000  Vocabulary Size:  2667  L2 Accuracy: 0.69971  L2 C:    0.50000\n",
      "L1 C:   16.00000  Vocabulary Size:  2827  L2 Accuracy: 0.69986  L2 C:    0.50000\n",
      "L1 C:   32.00000  Vocabulary Size:  3020  L2 Accuracy: 0.69957  L2 C:    0.50000\n"
     ]
    }
   ],
   "source": [
    "this_train_data = mini_train_data\n",
    "this_train_labels = mini_train_labels\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "train_docterm = vectorizer.fit_transform(this_train_data)\n",
    "total_vocab = vectorizer.vocabulary_\n",
    "total_vocab_byidx = dict((v,k) for k,v in vectorizer.vocabulary_.iteritems())\n",
    "\n",
    "vocab_sizes = []\n",
    "accuracies = []\n",
    "\n",
    "for c_l1 in pow(2.0,np.arange(12)-6):\n",
    "\n",
    "    # Train LogisticRegression with L1 penalty and specific C\n",
    "    lr_l1 = LogisticRegression(penalty='l1', tol=.01, C=c_l1)\n",
    "    lr_l1.fit(train_docterm, this_train_labels)\n",
    "\n",
    "    # Create a pruned vocabulary based on non-zero features from LogisticRegression with L1 penalty\n",
    "    pruned_vocab = set()\n",
    "    for f in lr_l1.coef_:\n",
    "        pruned_vocab.update([total_vocab_byidx[i] for i in np.flatnonzero(f)])\n",
    "\n",
    "    # Create a pruning vectorizer\n",
    "    pruning_vectorizer = CountVectorizer(vocabulary=pruned_vocab)\n",
    "    pruned_train_docterm = pruning_vectorizer.fit_transform(this_train_data)\n",
    "\n",
    "    # Train LogisticRegression with L2 penalty and pruned vocabulary\n",
    "    c = pow(2.0,np.arange(12)-12)\n",
    "    parameters = {'penalty' : ['l2'], 'C': c}\n",
    "    lr = LogisticRegression()\n",
    "    lr_clf = GridSearchCV(lr,parameters,scoring='accuracy')\n",
    "    lr_clf.fit(pruned_train_docterm, this_train_labels)\n",
    "    c_l2 = lr_clf.best_params_['C']\n",
    "\n",
    "    print \"L1 C: %10.5f  Vocabulary Size: %5d  L2 Accuracy: %.5f  L2 C: %10.5f\" % (c_l1, len(pruned_vocab), lr_clf.best_score_, c_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_submission_csv(classifier=None, data=None, predictions=None):\n",
    "    with open('submission.csv', 'wb') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['id', 'cuisine'])\n",
    "        if predictions is None:\n",
    "            predictions = classifier.predict(data)\n",
    "        for i in range(len(submission_data)):\n",
    "            csvwriter.writerow([submission_ids[i], predictions[i].strip()])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results in 0.78138 accuracy on Kaggle for 710th place with the full training set.\n"
     ]
    }
   ],
   "source": [
    "parse_all_data()\n",
    "\n",
    "this_train_data = full_train_data\n",
    "this_train_labels = full_train_labels\n",
    "\n",
    "def create_pruned_vectorizer(data, labels):\n",
    "    vectorizer = CountVectorizer()\n",
    "    train_docterm = vectorizer.fit_transform(data)\n",
    "    total_vocab = vectorizer.vocabulary_\n",
    "    total_vocab_byidx = dict((v,k) for k,v in vectorizer.vocabulary_.iteritems())\n",
    "\n",
    "    lr_l1 = LogisticRegression(penalty='l1', tol=.01, C=1.0)\n",
    "    lr_l1.fit(train_docterm, labels)\n",
    "\n",
    "    pruned_vocab = set()\n",
    "    for f in lr_l1.coef_:\n",
    "        pruned_vocab.update([total_vocab_byidx[i] for i in np.flatnonzero(f)])\n",
    "\n",
    "    pruning_vectorizer = CountVectorizer(vocabulary=pruned_vocab)\n",
    "    \n",
    "    return pruning_vectorizer\n",
    "\n",
    "pruning_vectorizer = create_pruned_vectorizer(this_train_data, this_train_labels)\n",
    "pruned_train_docterm = pruning_vectorizer.fit_transform(this_train_data)\n",
    "pruned_test_docterm = pruning_vectorizer.transform(submission_data)\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', C=0.5)\n",
    "lr.fit(pruned_train_docterm, this_train_labels)\n",
    "\n",
    "create_submission_csv(lr, pruned_test_docterm)\n",
    "\n",
    "print \"Results in 0.78138 accuracy on Kaggle for 710th place with the full training set.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Experiment - S1.*\n",
    "\n",
    "Idea: In this experiment we add simple ingredients to the text stream for a recipe and an indicator if a recipe contains meat, seafood or animal product. \n",
    "\n",
    "Outcome: Some minor increase in accuracy above the baseline was achieved in our Kaggle submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.985266757178\n",
      "Results in 0.78490 accuracy on Kaggle for 615th place when using full training set.\n"
     ]
    }
   ],
   "source": [
    "simple_ingredients = [\"chicken\", \"tomatoes\", \"rice\", \"garlic\", \"milk\", \"water\", \"cheese\", \"peanuts\",\n",
    "                      \"beef\", \"mushrooms\", \"pork\" ]  \n",
    "seafood_ingredients = [\"fish\", \"tuna\", \"salmon\", \"crab\", \"shrimp\", \"prawn\", \"calamari\", \"anchovy\"]\n",
    "meat_ingredients = [\"beef\", \"steak\", \"chicken\", \"pork\", \"bacon\", \"ham\",  \"turkey\", \"meat\"]\n",
    "pasta_ingredients = [\"tortellini\",\"fettucine\",\"linguine\",\"spaghetti\",\"rotini\",\"spaghettini\",\"diatlini\",\"penne\",\n",
    "                     \"macaroni\",\"ziti\",\"lasagna\",\"capellini\",\"fusilli\",\"pappardelle\",\"tagliatelle\",\"canneloni\",\n",
    "                     \"manicotti\",\"rigatoni\"]\n",
    "animal_product_ingredients = [\"egg\", \"butter\", \"milk\", \"cheese\"]\n",
    "\n",
    "def add_contains_feature(ingredients, contains_feature_name, recipe):\n",
    "    for ing in ingredients:\n",
    "        if ing in recipe:\n",
    "            recipe += \" \" + contains_feature_name\n",
    "    return recipe\n",
    "\n",
    "def add_simple_ingredients_features(ingredient_list):\n",
    "    for ing in simple_ingredients:\n",
    "        if ing in ingredient_list:\n",
    "            ingredient_list += \" \" + ing + \" \"\n",
    "    return ingredient_list\n",
    "\n",
    "def add_ingredient_group_features(ingredient_list):\n",
    "    ingredient_list = add_contains_feature(seafood_ingredients, \"contains_seafood\", ingredient_list)\n",
    "    ingredient_list = add_contains_feature(meat_ingredients, \"contains_meat\", ingredient_list)\n",
    "    #ingredient_list = add_contains_feature(pasta_ingredients, \"contains_italian_pasta\", ingredient_list)\n",
    "    # This next one decreases our accuracy a little.\n",
    "    # ingredient_list = add_contains_feature(animal_product_ingredients, \"contains_animal_product\", ingredient_list)\n",
    "    return ingredient_list\n",
    "\n",
    "def add_experimentS1_features(ingredient_list):\n",
    "    ingredient_list = add_simple_ingredients_features(ingredient_list)\n",
    "    ingredient_list = add_ingredient_group_features(ingredient_list)\n",
    "    return ingredient_list\n",
    "    \n",
    "parse_all_data(add_experimentS1_features)\n",
    "this_train_data = full_train_data\n",
    "this_train_labels = full_train_labels\n",
    "\n",
    "lr = LogisticRegression(C=1.0)\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "# achieved exact same accuracy with the pruned version of our vocabulary\n",
    "#vectorizer = create_pruned_vectorizer(this_train_data, this_train_labels)\n",
    "train_docterm = vectorizer.fit_transform(this_train_data)\n",
    "test_docterm = vectorizer.transform(full_train_data)\n",
    "\n",
    "lr.fit(train_docterm, this_train_labels)\n",
    "predictions = lr.predict(test_docterm)\n",
    "print np.mean(full_train_labels == predictions)\n",
    "\n",
    "create_submission_csv(lr, vectorizer.transform(submission_data))\n",
    "\n",
    "print \"Results in 0.79083 accuracy on Kaggle for 400th place when using full training set.\"\n",
    "#0.98446 - highest seen on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Experiment - S2.*\n",
    "\n",
    "Idea: Use PCA to reduce features then train on reduced feature set.\n",
    "\n",
    "Outcome: Because we vectorize the the ingredients list the data is very sparse. PCA does not run on a sparse matrix. Converting the sparse matrix to a dense array causes python to crash. Probably we run out of memory. Perhaps this process could be tried on a machine with more memory or perhaps PCA isn't appropriate for sparse features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parse_all_data()\n",
    "this_train_data = train_data\n",
    "this_train_labels = train_labels\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "train_docterm = vectorizer.fit_transform(this_train_data)\n",
    "test_docterm = vectorizer.transform(test_data)\n",
    "\n",
    "# THIS WILL TAKE FOREVER TO RUN. I wouldn't try it.\n",
    "#pca = PCA(n_components=20).fit(train_docterm.toarray())\n",
    "#for k in range(1,51):\n",
    "#    explained_variance = sum(pca.explained_variance_ratio_[0:k])\n",
    "#    print \"Variance Explained by first %s components: %s\" % (k, explained_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Experiment - S4.*\n",
    "\n",
    "Idea: For many recipes the most \"important\" ingredient is listed first. We'll try to give extra weight to the first word or first couple of words.\n",
    "\n",
    "Outcome: Submission accuracy is slightly worse than the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.755820385176\n",
      "Results in 0.76398 accuracy on Kaggle for 851th place when using full training set.\n"
     ]
    }
   ],
   "source": [
    "def duplicate_first_ingredient(ingredients_list):\n",
    "    first = ingredients_list.split(\" \")[0]\n",
    "    return ingredients_list + \" \" + first\n",
    "    \n",
    "parse_all_data(duplicate_first_ingredient)\n",
    "this_train_data = train_data\n",
    "this_train_labels = train_labels\n",
    "\n",
    "lr = LogisticRegression(C=1.0)\n",
    "vectorizer = create_pruned_vectorizer(this_train_data, this_train_labels)\n",
    "train_docterm = vectorizer.fit_transform(this_train_data)\n",
    "test_docterm = vectorizer.transform(test_data)\n",
    "\n",
    "lr.fit(train_docterm, this_train_labels)\n",
    "predictions = lr.predict(test_docterm)\n",
    "print np.mean(test_labels == predictions)\n",
    "#create_submission_csv(lr, vectorizer.transform(submission_data))\n",
    "\n",
    "print \"Results in 0.76398 accuracy on Kaggle for 851th place when using full training set.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Experiment - S5.\n",
    "\n",
    "Idea: Play with RandomForests and AdaBoost.\n",
    "\n",
    "Outcome: Neither produced stellar accuracy, but RandomForests did perform substantially better than a single decision tree so we now use it as our third in the majority vote predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 40}\n",
      "0.682355307487\n",
      "{'n_estimators': 50}\n",
      "0.472620304722\n"
     ]
    }
   ],
   "source": [
    "parse_all_data()\n",
    "this_train_data = train_data\n",
    "this_train_labels = train_labels\n",
    "\n",
    "vectorizer = create_pruned_vectorizer(this_train_data, this_train_labels)\n",
    "train_docterm = vectorizer.fit_transform(this_train_data)\n",
    "test_docterm = vectorizer.transform(test_data)\n",
    "\n",
    "n_estimators = 35\n",
    "params = {\"n_estimators\" : [30,31,35,40]}\n",
    "grid_search = GridSearchCV(RandomForestClassifier(), params, scoring='f1_micro')\n",
    "grid_search.fit(train_docterm, this_train_labels)   \n",
    "print grid_search.best_params_\n",
    "n_estimators = grid_search.best_params_[\"n_estimators\"]\n",
    "rfc = RandomForestClassifier(n_estimators=n_estimators)\n",
    "\n",
    "rfc.fit(train_docterm, this_train_labels)\n",
    "predictions = rfc.predict(test_docterm)\n",
    "print np.mean(test_labels == predictions)\n",
    "\n",
    "n_estimators = 50\n",
    "params = {\"n_estimators\" : [40,50,60]}\n",
    "grid_search = GridSearchCV(AdaBoostClassifier(), params, scoring='f1_micro')\n",
    "grid_search.fit(train_docterm, this_train_labels)   \n",
    "print grid_search.best_params_\n",
    "n_estimators = grid_search.best_params_[\"n_estimators\"]\n",
    "\n",
    "abc = AdaBoostClassifier(n_estimators=n_estimators)\n",
    "abc.fit(train_docterm, this_train_labels)\n",
    "predictions = abc.predict(test_docterm)\n",
    "print np.mean(test_labels == predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Experiment - S3.*\n",
    "\n",
    "Idea: Run multipe classifiers and see if a majority rules can produce better results.\n",
    "\n",
    "Outcome: KNearest neighbor is pretty bad and slow. Decision Tree results are not great, better than a flip of a coin and much quicker than KNearest neighbors. The end result is with Logistic Regression, Multinomial Bayes and a very poor performing decision tree is worse performance than the baseline. After doing testing on RandomForest it performed better than a standard decision tree so trying the majoirty rules with it as the third. Using RandomForest as the third upped our Kaggle accuracy by about 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.868859053653\n",
      "0.821340574244\n",
      "0.999195454317\n",
      "Majority rules:  0.926032081259\n",
      "Results in 0.76398 accuracy on Kaggle for 851th place when using full training set.\n"
     ]
    }
   ],
   "source": [
    "def majority_predictions(one, two, three):\n",
    "    new_predictions = []\n",
    "    for i, value in enumerate(one):\n",
    "        if value == two[i]:\n",
    "            new_predictions.append(value)\n",
    "        else:\n",
    "            new_predictions.append(three[i])\n",
    "    return new_predictions\n",
    "\n",
    "    \n",
    "parse_all_data()\n",
    "this_train_data = full_train_data\n",
    "this_train_labels = full_train_labels\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "train_docterm = vectorizer.fit_transform(this_train_data)\n",
    "test_docterm = vectorizer.transform(test_data)\n",
    "\n",
    "#lr = LogisticRegression(C=1.0)\n",
    "lr.fit(train_docterm, this_train_labels)\n",
    "lr_predictions = lr.predict(test_docterm)\n",
    "print np.mean(test_labels == lr_predictions)\n",
    "\n",
    "\n",
    "# find optimal alpha for multinomial naive bayes\n",
    "alphas = {\"alpha\" : [.00001, .0001, .001, .01, .1, 1, 10, 100, 1000]}\n",
    "grid_search = GridSearchCV(MultinomialNB(), alphas, scoring='f1_micro')\n",
    "grid_search.fit(train_docterm, this_train_labels)   \n",
    "\n",
    "# Calculate f1 score for best alpha\n",
    "mnb = MultinomialNB(alpha=grid_search.best_params_['alpha'])\n",
    "mnb.fit(train_docterm, this_train_labels)\n",
    "mnb_predictions = mnb.predict(test_docterm)\n",
    "print np.mean(test_labels == mnb_predictions)\n",
    "\n",
    "#k_values = {\"n_neighbors\" : range(1, 20)}\n",
    "#grid_search = GridSearchCV(KNeighborsClassifier(), k_values, scoring='f1_micro')\n",
    "#grid_search.fit(train_docterm, this_train_labels)\n",
    "#print \"best n\", grid_search.best_params_['n_neighbors']\n",
    "#best_n = grid_search.best_params_['n_neighbors']\n",
    "#best_n = 16 \n",
    "\n",
    "# Calculate f1 score for best k\n",
    "# Two slow and poor accuracy\n",
    "#knc = KNeighborsClassifier(n_neighbors=best_n)\n",
    "#knc.fit(train_docterm, this_train_labels)\n",
    "#predictions = knc.predict(test_docterm)\n",
    "#print np.mean(test_labels == predictions)\n",
    "\n",
    "\n",
    "#params = {\"min_samples_split\" : [2,3,4,5,6,7], \"min_samples_leaf\" : [1,2,3,4,5]}\n",
    "#grid_search = GridSearchCV(DecisionTreeClassifier(), params, scoring='f1_micro')\n",
    "#grid_search.fit(train_docterm, this_train_labels)   \n",
    "#print grid_search.best_params_\n",
    "\n",
    "#dtc = DecisionTreeClassifier(min_samples_split=4, min_samples_leaf=1)\n",
    "#dtc.fit(train_docterm, this_train_labels)\n",
    "#dtc_predictions = dtc.predict(test_docterm)\n",
    "#print np.mean(test_labels == dtc_predictions)\n",
    "\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=35)\n",
    "rfc.fit(train_docterm, this_train_labels)\n",
    "rfc_predictions = rfc.predict(test_docterm)\n",
    "print np.mean(test_labels == rfc_predictions)\n",
    "\n",
    "predictions = majority_predictions(lr_predictions, mnb_predictions, rfc_predictions)\n",
    "print \"Majority rules: \", np.mean(test_labels == predictions)\n",
    "\n",
    "submission_docterm = vectorizer.transform(submission_data)\n",
    "lr_predictions = lr.predict(submission_docterm)\n",
    "mnb_predictions = mnb.predict(submission_docterm)\n",
    "#dtc_predictions = dtc.predict(submission_docterm)\n",
    "rfc_predictions = rfc.predict(submission_docterm)\n",
    "predictions = majority_predictions(lr_predictions, mnb_predictions, rfc_predictions)\n",
    "\n",
    "create_submission_csv(predictions=predictions)\n",
    "\n",
    "print \"Results in 0.77936 accuracy on Kaggle for 727th place when using full training set.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error analysis to try to determine what is being incorrectly guessed. Experiment S1 produced the best results and should therefore be used for error analysis. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
