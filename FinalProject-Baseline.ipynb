{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_ids, submission_data = [], []\n",
    "test_data, test_labels = [], []\n",
    "train_data, train_labels = [], []\n",
    "mini_train_data, mini_train_labels = [], []\n",
    "full_train_labels, full_train_data = [], []\n",
    "target_names, target_labels = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'irish', u'mexican', u'chinese', u'filipino', u'vietnamese', u'moroccan', u'brazilian', u'japanese', u'british', u'greek', u'indian', u'jamaican', u'french', u'spanish', u'russian', u'cajun_creole', u'thai', u'southern_us', u'korean', u'italian']\n",
      "(19887L,)\n"
     ]
    }
   ],
   "source": [
    "def parse_all_data(per_recipe_function=None):\n",
    "    global submission_ids, submission_data, test_data, test_labels, train_data, train_labels\n",
    "    global mini_train_data, mini_train_labels, full_train_labels, full_train_data, target_names, target_labels\n",
    "    def parse_data(key_name, raw_data):\n",
    "        keys, data = [], []\n",
    "        for recipe in raw_data:\n",
    "            keys.append(recipe[key_name])\n",
    "            ingredient_list = \" \".join([x.replace(\" \",\"_\") for x in recipe[\"ingredients\"]])\n",
    "            ingredient_list = re.sub(r'[^A-Za-z\\s_]', '', ingredient_list)\n",
    "            ingredient_list = re.sub(r'_+oz_', '', ingredient_list)\n",
    "            ingredient_list = re.sub(r' _+', '', ingredient_list)\n",
    "            # This adds the word *count* to the list of ingredients equal to the number of ingredients\n",
    "            # this results in a vectorization that includes a last feature that has a count\n",
    "            # equal to the number of ingredients.\n",
    "            ingredient_list += (\" *count*\" * len(ingredient_list.split(\" \")))  \n",
    "\n",
    "            if per_recipe_function is not None:\n",
    "                ingredient_list = per_recipe_function(ingredient_list)\n",
    "\n",
    "            data.append(ingredient_list)\n",
    "        return keys, data\n",
    "\n",
    "    with open('train.json') as json_train_data:\n",
    "        train_raw = json.load(json_train_data)\n",
    "\n",
    "    with open('test.json') as json_test_data:\n",
    "        test_raw = json.load(json_test_data)\n",
    "\n",
    "    full_train_labels, full_train_data = parse_data(\"cuisine\", train_raw)\n",
    "\n",
    "    target_names = list(set(full_train_labels))\n",
    "    full_train_labels = np.array(full_train_labels)\n",
    "\n",
    "    submission_ids, submission_data = parse_data(\"id\", test_raw)\n",
    "\n",
    "    num_test = len(full_train_labels)\n",
    "    test_data, test_labels = full_train_data[num_test/2:], full_train_labels[num_test/2:]\n",
    "    train_data, train_labels = full_train_data[:num_test/2], full_train_labels[:num_test/2]\n",
    "\n",
    "    mini_train_data = train_data[:7000]\n",
    "    mini_train_labels = train_labels[:7000]\n",
    "\n",
    "\n",
    "    \n",
    "parse_all_data()\n",
    "\n",
    "print target_names\n",
    "print train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'_lowfat_cottage_cheese', u'_lowfat_milk', u'_reducedfat_milk', u'a_taste_of_thai_rice_noodles', u'aai', u'abalone', u'abbamele', u'accent', u'accent_seasoning', u'achiote']\n"
     ]
    }
   ],
   "source": [
    "this_train_data = mini_train_data\n",
    "this_train_labels = mini_train_labels\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "train_docterm = vectorizer.fit_transform(this_train_data)\n",
    "\n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "print sorted(features)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.697142857143\n"
     ]
    }
   ],
   "source": [
    "this_train_data = mini_train_data\n",
    "this_train_labels = mini_train_labels\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "train_docterm = vectorizer.fit_transform(this_train_data)\n",
    "\n",
    "c = pow(2.0,np.arange(6)-6)\n",
    "parameters = {'C': c}\n",
    "lr = LogisticRegression()\n",
    "lr_clf = GridSearchCV(lr,parameters,scoring='accuracy')\n",
    "lr_clf.fit(train_docterm, this_train_labels)\n",
    "print lr_clf.best_score_\n",
    "\n",
    "#alpha = pow(2.0,np.arange(24)-12)\n",
    "#parameters = {'alpha': alpha}\n",
    "#mnb = MultinomialNB()\n",
    "#mnb_clf = GridSearchCV(mnb,parameters,scoring='accuracy')\n",
    "#mnb_clf.fit(train_docterm, train_labels)\n",
    "#print mnb_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 C:    0.01562  Vocabulary Size:    33  L2 Accuracy: 0.45657  L2 C:    0.50000\n",
      "L1 C:    0.03125  Vocabulary Size:    72  L2 Accuracy: 0.54571  L2 C:    0.50000\n",
      "L1 C:    0.06250  Vocabulary Size:   159  L2 Accuracy: 0.62657  L2 C:    0.50000\n",
      "L1 C:    0.12500  Vocabulary Size:   294  L2 Accuracy: 0.66971  L2 C:    0.50000\n",
      "L1 C:    0.25000  Vocabulary Size:   500  L2 Accuracy: 0.68971  L2 C:    0.50000\n",
      "L1 C:    0.50000  Vocabulary Size:   796  L2 Accuracy: 0.69929  L2 C:    0.50000\n",
      "L1 C:    1.00000  Vocabulary Size:  1248  L2 Accuracy: 0.70114  L2 C:    0.50000\n",
      "L1 C:    2.00000  Vocabulary Size:  2203  L2 Accuracy: 0.69857  L2 C:    0.50000\n",
      "L1 C:    4.00000  Vocabulary Size:  2494  L2 Accuracy: 0.69700  L2 C:    0.50000\n",
      "L1 C:    8.00000  Vocabulary Size:  2676  L2 Accuracy: 0.69743  L2 C:    0.50000\n",
      "L1 C:   16.00000  Vocabulary Size:  2842  L2 Accuracy: 0.69629  L2 C:    0.50000\n",
      "L1 C:   32.00000  Vocabulary Size:  3022  L2 Accuracy: 0.69671  L2 C:    0.50000\n"
     ]
    }
   ],
   "source": [
    "this_train_data = mini_train_data\n",
    "this_train_labels = mini_train_labels\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "train_docterm = vectorizer.fit_transform(this_train_data)\n",
    "total_vocab = vectorizer.vocabulary_\n",
    "total_vocab_byidx = dict((v,k) for k,v in vectorizer.vocabulary_.iteritems())\n",
    "\n",
    "vocab_sizes = []\n",
    "accuracies = []\n",
    "\n",
    "for c_l1 in pow(2.0,np.arange(12)-6):\n",
    "\n",
    "    # Train LogisticRegression with L1 penalty and specific C\n",
    "    lr_l1 = LogisticRegression(penalty='l1', tol=.01, C=c_l1)\n",
    "    lr_l1.fit(train_docterm, this_train_labels)\n",
    "\n",
    "    # Create a pruned vocabulary based on non-zero features from LogisticRegression with L1 penalty\n",
    "    pruned_vocab = set()\n",
    "    for f in lr_l1.coef_:\n",
    "        pruned_vocab.update([total_vocab_byidx[i] for i in np.flatnonzero(f)])\n",
    "\n",
    "    # Create a pruning vectorizer\n",
    "    pruning_vectorizer = CountVectorizer(vocabulary=pruned_vocab)\n",
    "    pruned_train_docterm = pruning_vectorizer.fit_transform(this_train_data)\n",
    "\n",
    "    # Train LogisticRegression with L2 penalty and pruned vocabulary\n",
    "    c = pow(2.0,np.arange(12)-12)\n",
    "    parameters = {'penalty' : ['l2'], 'C': c}\n",
    "    lr = LogisticRegression()\n",
    "    lr_clf = GridSearchCV(lr,parameters,scoring='accuracy')\n",
    "    lr_clf.fit(pruned_train_docterm, this_train_labels)\n",
    "    c_l2 = lr_clf.best_params_['C']\n",
    "\n",
    "    print \"L1 C: %10.5f  Vocabulary Size: %5d  L2 Accuracy: %.5f  L2 C: %10.5f\" % (c_l1, len(pruned_vocab), lr_clf.best_score_, c_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_submission_csv(classifier, data):\n",
    "    with open('submission.csv', 'wb') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['id', 'cuisine'])\n",
    "        for i in range(len(submission_data)):\n",
    "            csvwriter.writerow([submission_ids[i], \" \".join(classifier.predict(data[i])).strip()])    \n",
    "\n",
    "this_train_data = full_train_data\n",
    "this_train_labels = full_train_labels\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "train_docterm = vectorizer.fit_transform(this_train_data)\n",
    "total_vocab = vectorizer.vocabulary_\n",
    "total_vocab_byidx = dict((v,k) for k,v in vectorizer.vocabulary_.iteritems())\n",
    "\n",
    "lr_l1 = LogisticRegression(penalty='l1', tol=.01, C=1.0)\n",
    "lr_l1.fit(train_docterm, this_train_labels)\n",
    "\n",
    "pruned_vocab = set()\n",
    "for f in lr_l1.coef_:\n",
    "    pruned_vocab.update([total_vocab_byidx[i] for i in np.flatnonzero(f)])\n",
    "\n",
    "pruning_vectorizer = CountVectorizer(vocabulary=pruned_vocab)\n",
    "pruned_train_docterm = pruning_vectorizer.fit_transform(this_train_data)\n",
    "pruned_test_docterm = pruning_vectorizer.transform(submission_data)\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', C=0.5)\n",
    "lr.fit(pruned_train_docterm, this_train_labels)\n",
    "\n",
    "#create_submission_csv(lr, pruned_test_docterm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Experiment - S1.*\n",
    "\n",
    "In this experiment we add simple ingredients to the text stream for a recipe and an indicator if a recipe contains meat, seafood or animal product. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.770000502841\n"
     ]
    }
   ],
   "source": [
    "simple_ingredients = [\"chicken\", \"tomatoes\", \"rice\", \"garlic\", \"milk\", \"water\", \"cheese\", \"peanuts\",\n",
    "                      \"beef\", \"mushrooms\", \"pork\" ]  \n",
    "seafood_ingredients = [\"fish\", \"tuna\", \"salmon\", \"crab\", \"shrimp\", \"prawn\", \"calamari\", \"anchovy\"]\n",
    "meat_ingredients = [\"beef\", \"steak\", \"chicken\", \"pork\", \"bacon\", \"ham\",  \"turkey\", \"meat\"]\n",
    "animal_product_ingredients = [\"egg\", \"butter\", \"milk\", \"cheese\"]\n",
    "\n",
    "def add_contains_feature(ingredients, contains_feature_name, recipe):\n",
    "    for ing in ingredients:\n",
    "        if ing in recipe:\n",
    "            recipe += \" \" + contains_feature_name\n",
    "    return recipe\n",
    "\n",
    "def add_simple_ingredients_features(ingredient_list):\n",
    "    for ing in simple_ingredients:\n",
    "        if ing in ingredient_list:\n",
    "            ingredient_list += \" \" + ing + \" \"\n",
    "    return ingredient_list\n",
    "\n",
    "def add_ingredient_group_features(ingredient_list):\n",
    "    ingredient_list = add_contains_feature(seafood_ingredients, \"contains_seafood\", ingredient_list)\n",
    "    ingredient_list = add_contains_feature(meat_ingredients, \"contains_meat\", ingredient_list)\n",
    "#    ingredient_list = add_contains_feature(animal_product_ingredients, \"contains_animal_product\", ingredient_list)\n",
    "    return ingredient_list\n",
    "\n",
    "def add_experimentS1_features(ingredient_list):\n",
    "    ingredient_list = add_simple_ingredients_features(ingredient_list)\n",
    "    ingredient_list = add_ingredient_group_features(ingredient_list)\n",
    "    return ingredient_list\n",
    "    \n",
    "parse_all_data(add_experimentS1_features)\n",
    "this_train_data = train_data\n",
    "this_train_labels = train_labels\n",
    "\n",
    "lr = LogisticRegression(C=1.0)\n",
    "vectorizer = CountVectorizer()\n",
    "train_docterm = vectorizer.fit_transform(this_train_data)\n",
    "test_docterm = vectorizer.transform(test_data)\n",
    "\n",
    "lr.fit(train_docterm, this_train_labels)\n",
    "predictions = lr.predict(test_docterm)\n",
    "print np.mean(test_labels == predictions)\n",
    "#create_submission_csv(lr, vectorizer.transform(submission_data))\n",
    "#0.770000502841"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Experiment - S2.*\n",
    "\n",
    "Idea: Use PCA to reduce features then train on reduced feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Experiment - S3.*\n",
    "\n",
    "Idea: Run multipe classifiers and see if a majority rules can produce better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
